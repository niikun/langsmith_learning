{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing for Different Types of Runs\n",
    "LangSmithでは、Run（処理）の種類を分類し、それぞれの役割を指定できます。これは、@traceable デコレーターを使って設定可能です。以下は、サポートされているRunの種類とその概要です。\n",
    "\n",
    "### Runの種類  \n",
    "1.LLM（言語モデルの実行）\n",
    "\n",
    "- 目的: LLM（大規模言語モデル）を呼び出す処理を表します。\n",
    "- 用途例: テキスト生成、質問への回答、タスクの完了。\n",
    "- 具体例: OpenAIのGPTモデルにクエリを送信して応答を生成する。 \n",
    "\n",
    "2.Retriever（情報検索）\n",
    "\n",
    "- 目的: 知識ベース、データベース、または外部ソースから関連するドキュメントやデータを取得する処理。\n",
    "- 用途例: 質問への回答に必要なコンテキストや情報を検索する。\n",
    "- 具体例: ベクトルデータベースをクエリし、特定のトピックに関連する文書を取得する。  \n",
    "\n",
    "3.Tool（ツールの実行）\n",
    "\n",
    "- 目的: 外部APIや関数呼び出しを使用して、特定のアクションを実行する処理。\n",
    "- 用途例: 計算を行う、Webサービスとやり取りする、コマンドを実行する。\n",
    "- 具体例: 為替レート計算ツールを使用して通貨換算を実行する。  \n",
    "\n",
    "4.Chain（チェーン）\n",
    "\n",
    "- 目的: デフォルトのRunタイプで、複数のRunを組み合わせてより大きなプロセスを構築します。\n",
    "- 用途例: LLM呼び出しと情報検索を連携させたワークフローを作成する。\n",
    "- 具体例: ユーザー入力を解析し、情報を検索して最終応答を生成するプロセス。  \n",
    "\n",
    "5.Prompt（プロンプトの作成）  \n",
    "\n",
    "- 目的: LLMで使用するプロンプトを動的に生成する処理。\n",
    "- 用途例: ユーザーの入力に応じたプロンプトを作成する。\n",
    "- 具体例: 自然言語の質問を特定のフォーマットに変換する。  \n",
    "\n",
    "6.Parser（解析器）\n",
    "\n",
    "- 目的: 構造化データを抽出する処理。\n",
    "- 用途例: LLMの出力を解析して、JSONや特定の形式に変換する。\n",
    "- 具体例: LLMの応答から住所や日付情報を抽出してデータベースに保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-academy\"\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Runs for Chat Models\n",
    "LangSmithでは、LLMトレースの特別なレンダリングと処理機能を提供しています。この機能を最大限に活用するためには、LLMトレースを特定のフォーマットで記録する必要があります。\n",
    "\n",
    "### input  \n",
    "入力は、OpenAI互換形式のメッセージリストである必要があります。この形式はPythonの辞書（またはTypeScriptオブジェクト）として表現されます。\n",
    "各メッセージは、次の2つのキーを含む必要があります：\n",
    "\n",
    "- role : メッセージの役割（例: \"user\", \"assistant\", \"system\" など）\n",
    "- content: メッセージの内容\n",
    "\n",
    "### output  \n",
    "以下のいずれかのフォーマットで出力を提供できます：\n",
    "1.辞書/オブジェクト形式（choicesを含む）\n",
    "- キー: choices\n",
    "- 値: 辞書/オブジェクトのリスト\n",
    "- 各辞書/オブジェクトは、以下の構造を持つ必要があります：\n",
    "    - キー: message\n",
    "    - 値: role と content を含むメッセージオブジェクト  \n",
    "2.辞書/オブジェクト形式（messageを含む）  \n",
    "- キー: message\n",
    "- 値: role と content を含むメッセージオブジェクト  \n",
    "3.タプル/配列形式  \n",
    "2要素の配列またはタプル\n",
    "- 1つ目の要素: role\n",
    "- 2つ目の要素: content  \n",
    "4.辞書/オブジェクト形式（role と content を含む）  \n",
    "\n",
    "- 直接 role と content をキーとして持つ辞書/オブジェクト\n",
    "入力名:\n",
    "\n",
    "関数への入力は、messagesという名前で提供される必要があります。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 追加のメタデータフィールド  \n",
    "次のメタデータフィールドを提供すると、LangSmithがモデルを識別し、コストを計算するのに役立ちます。\n",
    "LangChainやOpenAIのラッパーを使用する場合、これらのフィールドは自動的に正しく設定されます：\n",
    "\n",
    "- ls_provider: モデルプロバイダーの名前（例: \"openai\", \"anthropic\" など）\n",
    "- ls_model_name: モデル名（例: \"gpt-4o-mini\", \"claude-3-opus-20240307\" など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'message': {'role': 'assistant',\n",
       "    'content': '喜んで。何時に予約したいですか？'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "inputs = [\n",
    "    {\"role\": \"system\",\"content\":\"あなたは親切なアシスタントです\"},\n",
    "    {\"role\": \"user\", \"content\": \"2人用の席を予約したい。\"}\n",
    "]\n",
    "\n",
    "output = {\n",
    "    \"choices\":[\n",
    "        {\n",
    "            \"message\": {\n",
    "                \"role\":\"assistant\",\n",
    "                \"content\":\"喜んで。何時に予約したいですか？\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "@traceable(run_type=\"llm\",metadata={\"is_provider\":\"human\",\"is_models\":\"niikun\"})\n",
    "def chat_model(messages:list):\n",
    "    return output\n",
    "\n",
    "chat_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Streaming LLM Runs  \n",
    "\n",
    "ストリーミングの場合、出力を「リデュース」して、非ストリーミングバージョンと同じ形式に統一することができます。この機能は現在、Pythonでのみサポートされています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'choices': [{'message': {'role': 'assistant',\n",
       "     'content': 'Hello, 2人用の席を予約したい。'}}]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _reduce_chunks(chunks:list):\n",
    "    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks]) \n",
    "    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"},\n",
    "    reduce_fn=_reduce_chunks\n",
    ")\n",
    "def my_streaming_chat_model(messages:list):\n",
    "    for chunk in [\"Hello, \"+messages[1][\"content\"]]:\n",
    "        yield {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": chunk\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "list(\n",
    "    my_streaming_chat_model(\n",
    "        [\n",
    "            {\"role\": \"system\",\"content\":\"あなたは親切なアシスタントです。ユーザーに挨拶してください\"},\n",
    "            {\"role\": \"user\", \"content\": \"2人用の席を予約したい。\"}\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
