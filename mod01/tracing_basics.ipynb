{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-academy\"\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing with @traceable  \n",
    "`@traceable` デコレーターは、LangSmith Python SDKからトレースをログに記録するためのシンプルな方法です。任意の関数に`@traceable`をデコレーターとして付けるだけで利用できます。\n",
    "\n",
    "このデコレーターは、関数が呼び出されるたびにランツリー（run tree）を作成し、現在のトレース内に挿入する仕組みです。関数の入力、名前、その他の情報がLangSmithにストリームとして送信されます。関数がエラーを発生させた場合やレスポンスを返した場合、その情報もツリーに追加され、LangSmithにパッチ更新されます。そのため、エラーの原因を特定し、診断することが可能になります。これらはすべてバックグラウンドスレッドで実行されるため、アプリケーションの実行がブロックされることはありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(\n",
    "        metadata={\"vectordb\":\"sklearn\"}\n",
    ")\n",
    "def retrieve_documents(question:str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable\n",
    "def generate_answer(question:str, documents:List[str]):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable(\n",
    "        metadata={\"model_name\":MODEL_NAME,\"model_provier\":MODEL_PROVIDER}\n",
    ")\n",
    "def call_openai(messages:List[dict],\n",
    "                model:str=MODEL_NAME,\n",
    "                temperature:float=0.0,):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )  \n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_answer(question, documents)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with the @traceable decorator, simply decorate any function you want to log traces for by adding `@traceable` above the function definition. Ensure that the LANGCHAIN_TRACING_V2 environment variable is set to 'true' and that you have the necessary API key configured. This setup allows you to log traces effectively when the decorated function is called.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with the @traceable decorator?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Metadata\n",
    "LangSmithは、トレースとともに任意のメタデータを送信する機能をサポートしています。  \n",
    "メタデータは、実行に付加できるキーと値のペアの集合です。メタデータを使用することで、実行を生成したアプリケーションのバージョンや、実行が生成された環境、または実行に関連付けたいその他の情報を保存することができます。タグと同様に、メタデータを使用してLangSmithのUIで実行をフィルタリングしたり、分析のために実行をグループ化することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question:str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_answer(question:str, documents:List[str]):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"以下の文脈をもとに質問に、日本語で答えてください。\\n\\n文脈: {formatted_docs} \\n\\n 質問: {question}\"}\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(messages:List[dict],\n",
    "                model:str=MODEL_NAME,\n",
    "                temperature:float=0.0,):\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )  \n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_answer(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@traceableデコレーターを使用してRunにメタデータを追加するには、特別なメタデータキー（session_id、thread_id、またはconversation_idのいずれか）をトレースに添付します。具体的な手順については、関連するドキュメントを参照してください。詳細な情報は、メタデータとタグに関するページで確認できます。\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I add Metadata to a Run with @traceable?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To add metadata at runtime, you can pass it in using the `langsmith_extra` parameter along with a unique `run_id`. For example, you can call your function like this: `rag(\"your question\", langsmith_extra={\"run_id\": run_id, \"metadata\": {\"user_id\": \"harrison\"}})`. This allows you to log information such as User ID dynamically during execution.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I add metadata at runtime?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n",
    "print(ai_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
